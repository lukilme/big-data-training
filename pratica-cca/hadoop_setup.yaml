---
- name: Configurar ambiente Hadoop
  hosts: all
  become: yes
  vars:
    hadoop_user: hadoop
    java_home: /usr/local/openjdk-8
    hadoop_home: /opt/hadoop
    hive_home: /opt/hive
    sqoop_home: /opt/sqoop
    spark_home: /opt/spark
    flume_home: /opt/flume

  tasks:
    - name: Criar usuário Hadoop
      user:
        name: "{{ hadoop_user }}"
        shell: /bin/bash
        create_home: yes
        state: present
    - name: Atualizar cache de pacotes (Debian/Ubuntu)
      apt:
        update_cache: yes
        cache_valid_time: 3600
      when: ansible_os_family == 'Debian'

    - name: Instalar MySQL/MariaDB
      package:
        name: "{{
          'mariadb-server' if ansible_os_family == 'Debian'
          else 'mariadb-server' if ansible_os_family == 'RedHat'
          else 'mysql-server'
        }}"
        state: present

    - name: Iniciar serviço MySQL/MariaDB
      service:
        name: "{{
          'mariadb' if ansible_os_family == 'Debian'
          else 'mariadb' if ansible_os_family == 'RedHat'
          else 'mysqld'
        }}"
        state: started
        enabled: yes

    - name: Definir senha do usuário Hadoop
      user:
        name: "{{ hadoop_user }}"
        password: "{{ 'hadoop' | password_hash('sha512') }}"

    - name: Iniciar serviço MySQL/MariaDB
      service:
        name: "{{
          'mariadb' if ansible_os_family == 'Debian'
          else 'mysqld' if ansible_os_family == 'RedHat'
          else 'mysql'
        }}"
        state: started
        enabled: yes

    - name: Mover arquivo JAR do SLF4J
      shell: "mv {{ hadoop_home }}/share/hadoop/common/lib/slf4j-reload4j-1.7.36.jar {{ hadoop_home }}/share/hadoop/common/"
      args:
        creates: "{{ hadoop_home }}/share/hadoop/common/slf4j-reload4j-1.7.36.jar"

    - name: Configurar variáveis de ambiente
      blockinfile:
        path: /etc/profile
        block: |
          export HADOOP_HOME={{ hadoop_home }}
          export HIVE_HOME={{ hive_home }}
          export SQOOP_HOME={{ sqoop_home }}
          export SPARK_HOME={{ spark_home }}
          export FLUME_HOME={{ flume_home }}
          export JAVA_HOME={{ java_home }}
          export PATH=$PATH:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$FLUME_HOME/bin
          export CLASSPATH=$HIVE_HOME/lib/*:$CLASSPATH
          export CLASSPATH=$HIVE_HOME/lib/hive-beeline-4.0.1.jar:$CLASSPATH

    - name: Configurar hadoop-env.sh
      blockinfile:
        path: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
        block: |
          export JAVA_HOME={{ java_home }}
          export HDFS_NAMENODE_USER={{ hadoop_user }}
          export HDFS_DATANODE_USER={{ hadoop_user }}
          export HDFS_SECONDARYNAMENODE_USER={{ hadoop_user }}
          export YARN_NODEMANAGER_USER={{ hadoop_user }}
          export YARN_RESOURCEMANAGER_USER={{ hadoop_user }}
          export PATH=$PATH:{{ hive_home }}/bin
          export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/tmp/sqoop-classes
          export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:{{ hive_home }}/lib/*
          export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:{{ sqoop_home }}/sqoop-1.4.7.jar

    - name: Configurar SSH para Hadoop
      shell: |
        ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
        cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
        chmod 600 ~/.ssh/authorized_keys
        ssh-keyscan -H localhost namenode datanode >> ~/.ssh/known_hosts
      args:
        executable: /bin/bash
      become_user: "{{ hadoop_user }}"

    - name: Copiar conector MySQL
      copy:
        src: /shared/other/mysql-connector-java-8.0.28.jar
        dest: "{{ hive_home }}/lib/"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"

    - name: Configurar MySQL
      shell: |
        mysql -u root -p <<EOF
        ALTER USER 'root'@'localhost' IDENTIFIED BY 'root';
        FLUSH PRIVILEGES;
        EOF
      args:
        executable: /bin/bash

    - name: Ajustar permissões diretórios
      file:
        path: "{{ item }}"
        owner: "{{ hadoop_user }}"
        group: "{{ hadoop_user }}"
        recurse: yes
      loop:
        - /opt/hadoop
        - /opt/sqoop
        - /opt/hive
        - /opt/spark
        - /opt/flume

    - name: Inicializar HDFS
      shell: |
        hdfs namenode -format -force
        {{ hadoop_home }}/sbin/stop-dfs.sh
        {{ hadoop_home }}/sbin/stop-yarn.sh
        {{ hadoop_home }}/sbin/start-dfs.sh
        {{ hadoop_home }}/sbin/start-yarn.sh
        hdfs dfs -mkdir -p /tmp /user/hive/warehouse
        hdfs dfs -chmod -R 1777 /tmp
        hdfs dfs -chmod -R 775 /user/hive/warehouse
      args:
        executable: /bin/bash
      become_user: "{{ hadoop_user }}"

    - name: Iniciar serviços
      shell: |
        schematool -initSchema -dbType derby --verbose
        hive --service metastore > /var/log/hive-metastore.log 2>&1 &
        {{ spark_home }}/sbin/start-history-server.sh
        flume-ng agent -n agent -f {{ flume_home }}/conf/flume.conf > /var/log/flume.log 2>&1 &
      args:
        executable: /bin/bash
      become_user: "{{ hadoop_user }}"

  handlers:
    - name: Reiniciar SSH
      service:
        name: ssh
        state: restarted

    - name: Reiniciar MySQL
      service:
        name: mysql
        state: restarted